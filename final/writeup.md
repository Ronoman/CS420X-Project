# Final Project: Physarum with a Kinect

For the final project, I decided to extend the physarum simulation. There are two primary components that I added. First, a second colony is simulated in red, which is attracted to itself and is repelled by the presence of the original colony. Second, a webcam feed is loaded as a texture into the simulation texture. The original pink/purple colony is attracted to values on the XY channel of this texture. I played around with a Kinect V2 sensor, and ran a virtual webcam that acted as a passthrough for a depth image. This produced fascinating results.

My initial physarum simulation had the ability to change the color of the mold, but the simulation would not function with certain colors. To start with my final project, I fixed this by moving the color mapping to the copy shader. By storing the strength of the chemical on one channel of the simulation texture, I could sample that strength and change the rendered color independently. Once this was implemented, the color of the primary colony could be changed without impacting the way that the simulation functioned.

The next step was to add in the second colony. The second colony's chemical strength is stored in the green channel of the simulation texture, so that it can exist along with the original colony's chemical. For each vertex to know whether it was colony A or B, extra data had to be uploaded to the GPU. Initially, I planned on storing this data interleaved with the data already being uploaded. However, I could not get this working. I ended up switching to a separate buffer that was only uploaded once. This was more efficient anyways, since this data did not change every frame of the simulation like the rest of the vertex data. Once this data was present in the simulation shader, I added extra logic into the main loop to change how each agent acted.

To get kinect data into the simulation, I needed to write some local scripts to convert the data. This took many hours to setup, since Kinects are not officially supported on Linux. After downloading and compiling multiple open source repos, reading through NVidia and Microsoft documentation, and crashing my desktop over 5 times (0.5GB/s memory leaks are no joke), I was successfully getting depth data through the use of the `freenect2` python library. I then use the `pyvirtualcam` library to upload this data to a virtual camera, provided by the `v4l2loopback` package. This webcam acted like any other, so the browser could load it in without a problem.

Overall, I'm very happy with how my simulation turned out. As you can see in the video, the resulting simulation does look like my room, but with the stunning visual effects of physarum simulations. If I had more time to experiment with it, I would run tests to see what kind of effects I could get if I changed how the depth texture affected the simulation. Some ideas include color, agent turn radius, and agent speed.